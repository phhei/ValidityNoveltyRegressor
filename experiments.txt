1. Validity-Novelty-Classification ist ohne prior background knowledge kaum machbar (sehr hartes Test-Set)
2. All for one: Man kann synthetisch Trainingsdaten erstellen, die helfen
   1. Je vielfältiger die synthetischen Quellen, desto besser
   2. Je mehr die synthetischen Daten, desto besser
3. Zusätzliche Techniken helfen das Ergebnis weiter zu pushen
   ~~1. sample-data-class-balancing (on/out)~~
   2. adaptive Weights (on/out)
   3. (smooth Val/nov in Trainingdata (on/out))
   4. (Aus dem active learning: warm-start)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Original stand-alone (local): 13.1pc - sagt (fast) immer valid + not novel vorher %%%
%%% V0.4.0: 120766: 22.6-23.5-24.9+ (validity-f1: 74.3/ novelty.f1: 17.8)             %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Was wir schon mal festsetzen können:
-t roberta-large -bs 8 -lr 3e-5 --equalize_source_distribution --save --analyse train dev test --clean --repetitions 12 --use_annotation_ValTest \#--include_topic\#--min_confidence\#majority\#--continuous_sample_weight  -s \#-\n#XXX\#--classes\#1\#1\#1\#-1\#-\1\#1\#-1\#-1\#--automatic_samples

-------------------------------------------------------------------------------------

2: All for one: Man kann synthetisch Trainingsdaten erstellen, die helfen
- ExplaGraphs: --use_ExplaGraphs
- IBM: --use_IBM
- Essays: --use_essays \#--include_samples_without_detail_annotation_info
- ARCT: --use_ARCT
- ARCTUKW: --use_ARCTUKW
- Original: --use_annotation_train \#--include_topic

F1 in \% (p=fooled by leaving out the premise (<-3\%), c=fooled out by leaving out the conclusion(<-3\%), +=better in validity(+5\%), ~=better in novelty(+5\%)):

V0.2.0->0.3.1                               100                 1.000               10.000              100.000
ExplaGraphs                              119811->07.7pc        119819->24.5         119827->18.6         119893->119907->13.1pc
ExplaGraphs+IBM                          119812->13.1pc        119820->23.4         119828->26.3c        119894->33.0
ExplaGraphs+Essays                       119813->13.1pc        120333->19.9         119830->24.9         119895->119956->13.1pc
ExplaGraphs+IBM+Essays                   119814->08.7pc~       119822->29.1~        119829->13.1pc       119896->119957->17.5
ARCT+ARCTUKW                             119252->17.2c+(693)   119262->08.3pc(702)  119273->36.0(709)    119294->23.8c+(715)
ARCT+ARCTUKW+IBM                         119253->18.3c(699)    119263->14.9pc+(703) 119274->35.6(710)    119295->119806->12.9pc
ARCT+ARCTUKW+Essays                      119254->08.4pc(700)   119264->22.9(704)    119275->34.2(711)    119396->25.2+(717)
ARCT+ARCTUKW+IBM+Essays                                                             119276->36.9(713)    119297->25.7+(720)
ExplaGraphs+ARCT+ARCTUKW                 119815->14.0Pc        119823->09.4PC       119831->36.6         119891->119908->13.1pc
ExplaGraphs+ARCT+ARCTUKW+IBM+Essays      119816->07.6pC        119824->24.1         119832->23.2         119892->119909->22.8
Original                                 local ->13.1pc        120736->30.1+        120734->28.1+(751)   119698->119799->32.0
Original+ExplaGraphs                     119818->13.0pC        119825->23.5+        119833->23.7+        119890->119906->13.1pc
Original+ARCT+ARCTUKW                    120732->15.3pC        120737->13.9C        120738->17.5         119697->119798->13.1pc
Original+ARCT+ARCTUKW+IBM+Essays         120731->13.1pc        120740->13.1pc       120739->20.1         119701->32.5+
all                                      119817->10.1PC        119826->13.6c        119834->29.7+        119835->119905->13.1pc

V0.4.0                                      100                 1.000               10.000              100.000
ExplaGraphs                              120829
ExplaGraphs+IBM
ExplaGraphs+Essays
ExplaGraphs+IBM+Essays
ARCT+ARCTUKW                             120830
ARCT+ARCTUKW+IBM                         120831
ARCT+ARCTUKW+Essays                      120832
ARCT+ARCTUKW+IBM+Essays                  120772                120773               120836
ExplaGraphs+ARCT+ARCTUKW                 120833                120834               120835
ExplaGraphs+ARCT+ARCTUKW+IBM+Essays      120771                120774               120826
Original                                 7.7-16.9-26.1c(p)     13.0-19.4-32.6cp     120828                120827
Original+ExplaGraphs                     8.7-16.0-22.0Cp       15.3-19.0-22.1c(p)   120825
Original+ARCT+ARCTUKW                   13.1-15.6-20.4cp       18.0-22.9-30.2(c)    120824*
Original+ARCT+ARCTUKW+IBM+Essays        13.1-17.8-27.0cp       18.1-21.3-27.3~(c)(p)120808
all                                     13.1-15.6-18.1+cp      16.4-20.0-25.0(c)(p) 120809

Best:
- Original -> 10.000
- Original+ARCT+ARCTUKW+IBM+Essays -> 10.000
- ARCT+ARCTUKW+IBM+Essays -> 10.000
-------------------------------------------------------------------------------------

3. Zusätzliche Techniken helfen das Ergebnis weiter zu pushen:

- IBM: --use_IBM \#--continuous_sample_weight\#--continuous_val_nov
- Essays: --use_essays \#--include_samples_without_detail_annotation_info\#--continuous_sample_weight\#--continuous_val_nov
- ARCT: --use_ARCT \#-s\#all\#--continuous_sample_weight\#--continuous_val_nov
- ARCTUKW: --use_ARCTUKW \#--continuous_sample_weight\#--continuous_val_nov
- Original: --use_annotation_train \#--include_topic\#--continuous_sample_weight\#--continuous_val_nov

F1 in \% (p=fooled by leaving out the premise (<-3\%), c=fooled out by leaving out the conclusion(<-3\%), +=better in validity(+5\%), ~=better in novelty(+5\%)):

                                                adaptive Weights        smooth Val/nov          adaptive Weights+smooth Val/Nov
Original -> 10.000                              120467->31.7+           120468->40.0            120466->35.7
Original+ARCT+ARCTUKW+IBM+Essays -> 10.000
ARCT+ARCTUKW+IBM+Essays -> 10.000